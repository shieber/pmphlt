---
layout: post
title: No, the Turing Test has not been passed.
date: 2014-06-10 11:26:52.000000000 -04:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Alan Turing
- computer science
- language
tags: []
meta:
  _edit_last: '2110'
  _wp_rp_related_posts_query_result_cache_expiration: '1419920769'
  _wp_rp_related_posts_query_result_cache_6: a:12:{i:0;O:8:"stdClass":2:{s:7:"post_id";s:4:"1481";s:5:"score";s:16:"33.4801188371347";}i:1;O:8:"stdClass":2:{s:7:"post_id";s:4:"1538";s:5:"score";s:16:"18.6025826942072";}i:2;O:8:"stdClass":2:{s:7:"post_id";s:4:"1203";s:5:"score";s:16:"16.1140383539959";}i:3;O:8:"stdClass":2:{s:7:"post_id";s:4:"2151";s:5:"score";s:16:"15.5386742090924";}i:4;O:8:"stdClass":2:{s:7:"post_id";s:4:"2092";s:5:"score";s:16:"11.4067578268387";}i:5;O:8:"stdClass":2:{s:7:"post_id";s:4:"1376";s:5:"score";s:16:"11.4067578268387";}i:6;O:8:"stdClass":2:{s:7:"post_id";s:4:"1703";s:5:"score";s:16:"11.1288196624825";}i:7;O:8:"stdClass":2:{s:7:"post_id";s:4:"1574";s:5:"score";s:16:"11.1288196624825";}i:8;O:8:"stdClass":2:{s:7:"post_id";s:3:"884";s:5:"score";s:16:"11.1288196624825";}i:9;O:8:"stdClass":2:{s:7:"post_id";s:4:"2202";s:5:"score";s:16:"9.92957741785487";}i:10;O:8:"stdClass":2:{s:7:"post_id";s:4:"1829";s:5:"score";s:16:"9.87224752097995";}i:11;O:8:"stdClass":2:{s:7:"post_id";s:3:"937";s:5:"score";s:16:"9.87224752097995";}}
  _jetpack_related_posts_cache: a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1647974064;s:7:"payload";a:0:{}}}
author:
  login: shieber
  email: shieber@seas.harvard.edu
  display_name: Stuart Shieber
  first_name: Stuart
  last_name: Shieber
permalink: "/pamphlet/2014/06/10/no-the-turing-test-has-not-been-passed/"
---
<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body></p>
<p>[mathjax]</p>
<!-- <table width="200" align="right" bgcolor="#F7EFE5">
<tbody>
<tr>
<td align="center"><a title="Shelf of journals" href="http://xkcd.com/329/"><img src="{{site.baseurl}}/assets/2014/06/turing_test.png" alt="Turing Test" width="200"></a></td>
</tr>
<tr>
<td style="text-align: center">
<span style="color: #999999">…that's not Turing's Test…</span><br />
<span style="color: #999999;font-size: 85%">“<a href="http://xkcd.com/329/">Turing Test</a>” image from <a href="http://xkcd.com/">xkcd</a>. Used <a href="http://creativecommons.org/licenses/by-nc/2.5/">by permission</a>.</span>
</td>
</tr>
</tbody>
</table> -->
<figure class="wrapped-image">
  <img src="{{site.baseurl}}/assets/2014/06/turing_test.png" alt="Turing Test" />
  <figcaption>…that's not Turing's Test…</figcaption>
</figure>

<p>There has been a flurry of interest in the Turing Test in the last few days, precipitated by a claim that (at last!) a program has passed the Test. The program in question is called "Eugene Goostman" and the claim is promulgated by Kevin Warwick, a professor of cybernetics at the University of Reading and organizer of a recent chatbot competition there.</p>
<p>The Turing Test is a topic that I have a deep interest in (see <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:2027203">this</a>, and <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:2032677">this</a>, and <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:2252596">this</a>, and <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:5343165">this</a>, and, most recently, <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:11684156">this</a>), so I thought to give my view on <a href="http://www.reading.ac.uk/news-and-events/releases/PR583836.aspx">Professor Warwick's claim</a> "We are therefore proud to declare that Alan Turing's Test was passed for the first time on Saturday." The main points are these. The Turing Test was not passed on Saturday, and "Eugene Goostman" seems to perform qualitatively about as poorly as many other chatbots in emulating human verbal behavior. In summary: There's nothing new here; move along.</p>
<p>First, the Turing Test that Turing had in mind was a criterion of <em>indistinguishability</em> in verbal performance between human and computer in an open-ended wide-ranging interaction. In order for the Test to be passed, judges had to perform <em>no better than chance</em> in unmasking the computer. But in the recent event, the interactions were quite time-limited (only five minutes) and in any case, the purported Turing-Test-passing program was identified correctly more often than not by the judges (almost 70% of the time in fact). That's not Turing's test.</p>
<blockquote><p>
<strong>Update June 17, 2014</strong>: The time limitation was even worse than I thought. According to my colleague <a href="http://www.cs.vassar.edu/~hunsberg/">Luke Hunsberger</a>, computer science professor at Vassar College, who was a judge in this event, the five minute time limit was for <em>two</em> simultaneous interactions. Further, there were often substantial response delays in the system. In total, he estimated that a judge might average only four or five rounds of chat with each interlocutor. I’ve argued before that <a href="http://bit.ly/1nya270">a grossly time-limited Turing Test is no Turing Test at all</a>.</p></blockquote>
<p>Sometimes, people trot out the prediction from Turing's seminal 1950 <em>Mind</em> article that "I believe that in about fifty years’ time it will be possible to programme computers, with a storage capacity of about \(10^9\), to make them play the imitation game so well that an average interrogator will not have more than 70 per cent. chance of making the right identification after five minutes of questioning." As I explain <a href="http://www.amazon.com/The-Turing-Test-Behavior-Intelligence/dp/0262692937/">in my book on the Test</a>:</p>
<blockquote><p>The first thing to note about the prediction is that it is not a prediction about the Test per se: Turing expects 70 percent prediction accuracy, not the more difficult 50 percent expected by chance, and this after only a limited conversation of five minutes. He is therefore predicting passage of a test much simpler than the Test.</p></blockquote>
<blockquote><p>Not only does the prediction not presuppose a full Turing Test, but it could well be argued that it had already come to pass with the advent of Joseph Weizenbaum’s Eliza some thirty-five years early. Weizenbaum developed a simple computer program that attempted to imitate the parroting conversational style of a Rogerian psychotherapist.... Although the methods used were quite simple – repeating the user’s question after adjusting some pronouns, throwing in a stock phrase every now and then – the result was, in its own way, extremely convincing.</p></blockquote>
<p>Second, "Eugene Goostman" uses the very techniques that began with Weizenbaum's  <a href="https://en.wikipedia.org/wiki/ELIZA">"Eliza" program</a> from the 1960's. We see the same tricks – repeating the judge's statements with simple substitutions, keyword-triggered responses, falling back on vague or unresponsive replies, and the like. Those tricks are no more successful than they have been in the two decades of runnings of the Loebner Prize Competition, another <a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:2032677">ill-conceived attempt at running a Turing-like test</a>. And there too, entrants used the trick of having their programs emulate humans with built in excuses.  "Eugene Goostman" purports to be a non-English-fluent child, rather than a fluent adult.<sup><a id="fnref1" class="footnoteRef" href="#fn1">1</a></sup></p>
<p>In fact, anyone with the slightest familiarity with chatbots, the modern incarnations of "Eliza", could unmask "Eugene Goostman" quickly. (The judges at the recent competition were apparently not so familiar.) Scott Aaronson has <a href="http://www.scottaaronson.com/blog/?p=1858">provided a transcript</a> of his own interaction with "Eugene Goostman", which shows the telltale signs as soon as the computer's first reply, which blatantly ignores an utterly trivial question and tries to change the subject:</p>
<blockquote><p>Scott: Which is bigger, a shoebox or Mount Everest?</p>
<p>Eugene: I can’t make a choice right now. I should think it out later. And I forgot to ask you where you are from…</p></blockquote>
<p>Here's an appeal to cute, vague language:</p>
<blockquote><p>Scott: How many legs does a camel have?</p>
<p>Eugene: Something between 2 and 4. Maybe, three? :-))) By the way, I still don’t know your specialty – or, possibly, I’ve missed it?</p></blockquote>
<p>And here's the repetition of a canned response:</p>
<blockquote><p>Scott: No, I need to know that you’re not a chatbot. Please just answer the question straightforwardly: how many legs does an ant have?</p>
<p>Eugene: Something between 2 and 4. Maybe, three? :-))) Oh, what a fruitful conversation;-)</p></blockquote>
<p>Here's a canned keyword-triggered response, nonresponsive as usual:</p>
<blockquote><p>Scott: In your opinion, does your existence demonstrate any idea or principle that wasn’t demonstrated just as convincingly by ELIZA, Joseph Weizenbaum’s chatbot from the 1960s?</p>
<p>Eugene: Eliza was a break-thru. All the bots after it were nothing but weak parodies, claiming to have “revolutionary improvements”.</p></blockquote>
<p>And here's "Eliza"'s trick of rewriting the judge's utterance after pronoun replacement and (poorly executed) synonym substitution:</p>
<blockquote><p>Scott: OK, I’ll count that as the second sensible thing you’ve said (grading generously). Don’t you think that a more convincing chatbot could be created, by using Google to trawl the entire web for plausible-sounding answers (analogous to what IBM’s Watson did)?</p>
<p>Eugene: Yes, I do think that a more convincing chatbot can be built by using google to trawl the all web for plausible – sounding – I hope I have right to have my own opinion. Maybe, let’s talk about something else? What would you like to discuss?</p></blockquote>
<p>Literally every one of "Eugene"'s responses reflects its "Eliza"-like programming. It would be amusing, if it weren't so predictable.</p>
<p>In summary, "Eugene Goostman" is not qualitatively superior to other chatbots, and certainly has not passed a true Turing Test. It isn't even close.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn1">In a parody of this approach, the late John McCarthy, professor of computer science at Stanford University and inventor of the term "artifical intelligence", wrote a letter to the editor responding to a publication about an "Eliza"-like program that claimed to emulate a paranoid psychiatric patient. He presented his own experiments that I described in <a href="http://www.amazon.com/The-Turing-Test-Behavior-Intelligence/dp/0262692937/">my Turing Test book</a>: "He had designed an even better program, which passed the same test. His also had the virtue of being a very inexpensive program, in these times of tight money. In fact you didn’t even need a computer for it. All you needed was an electric typewriter. His program modeled infantile autism. And the transcripts – you type in your questions, and the thing just sits there and hums – cannot be distinguished by experts from transcripts of real conversations with infantile autistic patients."<a href="#fnref1">↩</a>
</li>
</ol>
</div>
<p></body></html></p>
